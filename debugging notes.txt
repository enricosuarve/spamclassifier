if I remove line limiting to unique rows (line 144), I get an error:
Traceback (most recent call last):
  File "C:\Users\snpre\Google Drive\Bath CompSci MSc\Yr2 AI\Graded Assignment 3 Spam Filtering\spamclassifier\spamclassifier.py", line 329, in <module>
    run_tests()
  File "C:\Users\snpre\Google Drive\Bath CompSci MSc\Yr2 AI\Graded Assignment 3 Spam Filtering\spamclassifier\spamclassifier.py", line 304, in run_tests
    predictions = classifier.predict(test_data, True)
  File "C:\Users\snpre\Google Drive\Bath CompSci MSc\Yr2 AI\Graded Assignment 3 Spam Filtering\spamclassifier\spamclassifier.py", line 116, in predict
    class_predictions = self.parse_decision_tree(data)
  File "C:\Users\snpre\Google Drive\Bath CompSci MSc\Yr2 AI\Graded Assignment 3 Spam Filtering\spamclassifier\spamclassifier.py", line 256, in parse_decision_tree
    current_value = row[current_attribute]
IndexError: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices

above crash occurs as for some reason 0.0 has been entered in the decision tree as an attribute?

also decision tree seems to go in number order suspiciously frequently

as things stand, if I do not remove duplicate rows I get negative gains (is this correct?) and if there are only 2 remaining attributes by the time it gets to line 204 the max_gain table is still just full of the default 0.0 values; one of these gets transferred to the branch table which causes errors when parsing later on.
Need to either check if negative gains are a thing (if not calculation may nee improving), if so need a way to ensure best gain is always picked (maybe start with -1000000 at gian per attribute or an empty array?)

Checked and short answer is NO gain should never be negativehttps://www.google.com/url?sa=t&source=web&rct=j&url=https://stackoverflow.com/questions/3289589/can-the-value-of-information-gain-be-negative%23:~:text%3DFirst%252C%2520the%2520answer%2520is%2520no%252C%2520it%2520cannot%2520be%2520negative.&ved=2ahUKEwiK_PCTx_j4AhVXg1wKHdcOBZQQFnoECA4QBQ&usg=AOvVaw1x5IOEJJ67GRE1B5IyMaoM

the entropy calculation for s1 in calculate_gain was wrong at line 228

when removing duplicates has a higher accuracy of 83% (using training data for both training and testing)
70 out of 613 good emails were classed as spam (11.42%)
287 out of 387 spam emails were classed as spam (74.16%)
Accuracy on test data is: 0.83

check whole gain calculation next - compare calculate_gain to https://medium.com/analytics-vidhya/entropy-calculation-information-gain-decision-tree-learning-771325d16f
print("example gain = ", classifier.calculate_gain(2, 6, 3, 3, 8, 6, 5, 9, 14))
I get 0.8324.. - should be 0.048


found an issue with the gain calculation (brackets in the wrong place in lines 217 & 221)
corrected and massively increases accuracy testing training data against itself with duplicates removed (errors if leave in duplicates so check there is not something else that can be optimised)
5 out of 613 good emails were classed as spam (0.82%)
350 out of 387 spam emails were classed as spam (90.44%)
Accuracy on test data is: 0.958


added logic to always pick the first attribute if entropy for all remaining data is zero

Dupes OK training data for training and test data for testing:
40 out of 301 good emails were classed as spam (13.29%)
175 out of 199 spam emails were classed as spam (87.94%)
Accuracy on test data is: 0.872

no dupes duplicates allowed training data for training and testing:
5 out of 613 good emails were classed as spam (0.82%)
350 out of 387 spam emails were classed as spam (90.44%)
Accuracy on test data is: 0.958

Dupes OK training training data for training and testing:
29 out of 613 good emails were classed as spam (4.73%)
378 out of 387 spam emails were classed as spam (97.67%)
Accuracy on test data is: 0.962

No Dupes training data for training and test data for testing:
15 out of 301 good emails were classed as spam (4.98%)
163 out of 199 spam emails were classed as spam (81.91%)
Accuracy on test data is: 0.898

plan to improve
1) train decision tree on training data
2) apply decision tree to training data
3) append decision tree prediction to training data
4) train naive bayes
5) run decision tree on test data
6) apply decision tree to test data
7) run naive bayes on test data
result.....